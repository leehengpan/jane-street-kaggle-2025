{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob.glob(\"./input/train.parquet/partition_id=*/part-*.parquet\")\n",
    "# file_paths.sort()\n",
    "# lazy_dfs = []\n",
    "\n",
    "# for idx, file_path in enumerate(tqdm(file_paths)):\n",
    "#     lazy_df = pl.read_parquet(file_path).lazy().with_columns(\n",
    "#         pl.lit(idx).alias(\"partition_id\")\n",
    "#     )\n",
    "    \n",
    "#     lazy_dfs.append(lazy_df)\n",
    "\n",
    "\n",
    "# del lazy_df\n",
    "# gc.collect()\n",
    "# train_df = pl.concat(lazy_dfs)\n",
    "\n",
    "# del lazy_dfs\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset has around 47M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.scan_parquet(\"./preprocessed_dataset/training.parquet\")\n",
    "# train_df = train_df.sort([\"date_id\", \"time_id\", \"symbol_id\"])\n",
    "train_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of unique date_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1699 unique `date_id`s in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of unique date_id\n",
    "train_df.select(pl.col('date_id').unique()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of timesteps in a given day (across all symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_df.group_by(\"date_id\").agg(\n",
    "    pl.col(\"time_id\").count().alias(\"num_timesteps\")\n",
    ").sort(by=['date_id']).collect()\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_pandas().plot(x=\"date_id\", y=\"num_timesteps\")\n",
    "plt.title('Number of Timesteps by Date ID')\n",
    "plt.xlabel('Date ID')\n",
    "plt.ylabel('Number of Timesteps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of unique timesteps in a given day (across all symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_df.group_by(\"date_id\").agg(\n",
    "    pl.col(\"time_id\").n_unique().alias(\"num_unique_timesteps\")\n",
    ").sort(by=['date_id']).collect()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the above table into a line chart\n",
    "res.to_pandas().plot(x=\"date_id\", y=\"num_unique_timesteps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.group_by(\"num_unique_timesteps\").agg(\n",
    "    pl.col(\"date_id\").n_unique().alias(\"num_occurrences\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First half of the data (677 days) has 849 unique timesteps per day, and the second half (1022 days) has 968 unique timesteps per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of unique symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of unique symbol_ids\n",
    "train_df.select(pl.col('symbol_id').unique()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of symbols by `date_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of symbol_ids per date_id\n",
    "res = train_df.group_by(\"date_id\").agg(\n",
    "    pl.col(\"symbol_id\").n_unique().alias(\"num_unique_symbols\")\n",
    ").sort(by=['date_id']).collect()\n",
    "\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_pandas().plot(x=\"date_id\", y=\"num_unique_symbols\")\n",
    "plt.title('Number of Unique Symbols by Date ID')\n",
    "plt.xlabel('Date ID')\n",
    "plt.ylabel('Number of Unique Symbols')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of time_ids for each symbol_id\n",
    "res = train_df.group_by(\"symbol_id\").agg(\n",
    "    pl.col(\"time_id\").count().alias(\"num_timesteps\")\n",
    ").sort(by=['symbol_id']).collect()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_pandas().plot(x=\"symbol_id\", y=\"num_timesteps\", kind='bar')\n",
    "plt.title('Number of Timesteps by Symbol ID')\n",
    "plt.xlabel('Symbol ID')\n",
    "plt.ylabel('Number of Timesteps')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of timesteps by `date_id` and `symbol_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_df.group_by(\"date_id\", \"symbol_id\").agg(\n",
    "    pl.col(\"time_id\").count().alias(\"num_timesteps\")\n",
    ").sort(by=['date_id', 'symbol_id']).collect()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the number of timesteps by `date_id` and `symbol_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for symbol in [1, 20]:\n",
    "    symbol_data = res.filter(pl.col(\"symbol_id\") == symbol).to_pandas()\n",
    "    plt.plot(symbol_data[\"date_id\"], symbol_data[\"num_timesteps\"], label=f'Symbol {symbol}')\n",
    "\n",
    "plt.title('Timesteps for Symbol ID 1 and 20')\n",
    "plt.xlabel('Date ID')\n",
    "plt.ylabel('Number of Timesteps')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NaN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get group 8 as our analysis target\n",
    "sorted_df_symbol_id = train_df.filter(pl.col(\"symbol_id\") == 8).sort(by=['date_id', 'time_id'])\n",
    "sorted_df_symbol_id.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute statistics for NaN's, **=> NaN's occur in the beginning of time series for**\n",
    "   - **feature_21, feature_26, feature_27, feature_31**\n",
    "   - **feature_00, feature_01, feature_02, feature_03, feature_04**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Count of NaNs for each row\n",
    "feature_cols = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "sorted_df_symbol_id = sorted_df_symbol_id.with_columns(\n",
    "    pl.fold(\n",
    "        acc=pl.lit(0),  # Starting accumulator value\n",
    "        function=lambda acc, col: acc + col.is_null().cast(pl.Int32),  # Increment for null values\n",
    "        exprs=[pl.col(col) for col in feature_cols]  # List of columns to process\n",
    "    ).alias('feature_nan_count')\n",
    ")\n",
    "\n",
    "sorted_df_symbol_id.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Top 10 columns with most NaNs\n",
    "# column_nan_count = sorted_df_symbol_id.select(pl.col(feature_cols).null_count())\n",
    "# print(\"Top 10 columns with most NaNs\")\n",
    "# column_nan_count.collect().to_pandas().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Features without NaNs\n",
    "# features_without_nan = [\n",
    "#     col for col in feature_cols \n",
    "#     if sorted_df_symbol_id.select(pl.col(col).null_count().alias('nulls')).collect()['nulls'][0] == 0\n",
    "# ]\n",
    "# features_with_nans = [\n",
    "#     col for col in feature_cols \n",
    "#     if sorted_df_symbol_id.select(pl.col(col).null_count().alias('nulls')).collect()['nulls'][0] > 0\n",
    "# ]\n",
    "# print(\"features_without_nan\", features_without_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Calculate the percentage of NaNs for each feature\n",
    "# feature_nan_percentages = (\n",
    "#     sorted_df_symbol_id.select(\n",
    "#         [\n",
    "#             (pl.col(col).is_null() / sorted_df_symbol_id.collect().shape[0] * 100).alias(col)\n",
    "#             for col in feature_cols\n",
    "#         ]\n",
    "#     ).collect()\n",
    "#     .to_pandas()\n",
    "#     .mean()\n",
    "#     .sort_values(ascending=False)\n",
    "# )\n",
    "\n",
    "# # Display the top 10 features with the highest percentage of NaNs\n",
    "# print(\"Percentage of NaNs for each feature:\")\n",
    "# print(feature_nan_percentages.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the features with the highest percentage of NaNs\n",
    "# features_with_most_nan = feature_nan_percentages.head(10).index\n",
    "# result = sorted_df_symbol_id.select(features_with_most_nan)\n",
    "\n",
    "# result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualize correlation matrix between features in which NaN occurs, **=> NaN occurs at same timestamps for**\n",
    "   - **{feature_32-33, feature_58, feature_73-74}**\n",
    "   - **{feature_39, feature_42, feature_50, feature_53}**\n",
    "   - **{feature_40, feature_43}**\n",
    "   - **{feature_41, feature_44, feature_52, feature_55}**\n",
    "   - **{feature_45-46, feature_62-66}**\n",
    "   - **{feature_51, feature_54}**\n",
    "   - **{feature_75-76}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a boolean DataFrame of NaN values\n",
    "# nan_bool_df = sorted_df_symbol_id.select(features_with_nans).collect().to_pandas()\n",
    "\n",
    "# # Calculate correlation matrix\n",
    "# nan_corr_matrix = nan_bool_df.corr()\n",
    "\n",
    "# # Visualize\n",
    "# plt.figure(figsize=(12, 12)) \n",
    "# sns.heatmap(nan_corr_matrix, annot=True, cmap='coolwarm', fmt='.1f', vmin=-1, vmax=1, cbar=False)\n",
    "# plt.title('Correlation Matrix of NaN Values')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis\n",
    "1. Compute standard deviation for each feature on symbol_id=8, **=> feature_09, feature_10, feature_11 might depend on instrument since they have 0 stddev.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_std = sorted_df_symbol_id.select(feature_cols).collect().to_pandas().std()\n",
    "print(\"min\", feature_std.min())\n",
    "print(\"max\", feature_std.max())\n",
    "print(feature_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_std_mask = feature_std == 0\n",
    "feature_std[zero_std_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find maximum/minimum standard deviation for features on all `symbol_id`'s, then plot the features with symbol_id=8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation\n",
    "feature_std = train_df.select(feature_cols).collect().to_pandas().std()\n",
    "\n",
    "# Drop specific columns\n",
    "feature_std_filtered = feature_std.drop([\"feature_09\", \"feature_10\", \"feature_11\"])\n",
    "\n",
    "# Identify features with max and min std\n",
    "argmax_idx = feature_std_filtered.idxmax()\n",
    "argmin_idx = feature_std_filtered.idxmin()\n",
    "\n",
    "# Plot the values of these features\n",
    "max_std_values = sorted_df_symbol_id.select(argmax_idx).collect().to_pandas()\n",
    "min_std_values = sorted_df_symbol_id.select(argmin_idx).collect().to_pandas()\n",
    "\n",
    "plt.plot(max_std_values, label=f'{argmax_idx} (max std)')\n",
    "plt.plot(min_std_values, label=f'{argmin_idx} (min std)')\n",
    "\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(f\"Plot of {argmax_idx} (max std) and {argmin_idx} (min std)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature with max std (argmax_idx) separately\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sorted_df_symbol_id.select(argmax_idx).collect().to_pandas(), label=f'{argmax_idx} (max std)', color='blue')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(f\"Plot of {argmax_idx} (max std)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the feature with min std (argmin_idx) separately\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sorted_df_symbol_id.select(argmin_idx).collect().to_pandas(), label=f'{argmin_idx} (min std)', color='red')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(f\"Plot of {argmin_idx} (min std)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute other feature statistics on `symbol_id`=8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mean = sorted_df_symbol_id.select(feature_cols).collect().to_pandas().mean()\n",
    "print(feature_mean) \n",
    "print(\"min\", feature_mean.min())\n",
    "print(\"max\", feature_mean.max())\n",
    "print(feature_mean[[\"feature_09\",\"feature_10\",\"feature_11\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_max = sorted_df_symbol_id.select(feature_cols).collect().to_pandas().drop(columns=[\"feature_09\", \"feature_10\", \"feature_11\"]).max()\n",
    "print(feature_max) \n",
    "print(\"min\", feature_max.min())\n",
    "print(\"max\", feature_max.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_min = sorted_df_symbol_id.select(feature_cols).collect().to_pandas().drop(columns=[\"feature_09\", \"feature_10\", \"feature_11\"]).min()\n",
    "print(feature_min) \n",
    "print(\"min\", feature_min.min())\n",
    "print(\"max\", feature_min.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = sorted_df_symbol_id.select(feature_cols).collect().to_pandas().corr()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".1f\", \n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title(\"Correlation heatmap of features 00 to 78, symbol_id=8\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharey=True)\n",
    "\n",
    "for i, feature in enumerate(feature_list):\n",
    "    # Convert the column to a Pandas Series\n",
    "    pandas_series = train_df.select(feature).collect().to_pandas()[feature]\n",
    "    \n",
    "    # Count unique values\n",
    "    unique_values = pandas_series.value_counts()\n",
    "    print(f\"Number of unique values for {feature}: {len(unique_values)}\")\n",
    "    \n",
    "    # Create a bar plot\n",
    "    sns.barplot(x=unique_values.index, y=unique_values.values, color='skyblue', edgecolor='black', ax=axes[i])\n",
    "    axes[i].set_title(f\"Unique values for {feature}\")\n",
    "    axes[i].set_xlabel(\"UniqueValue\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How could *weight* be determined?\n",
    "1. Compute correlation between `weight` and each feature (Pearson correlation measures the linear relationship between two variables, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship; Spearman correlation is a non-parametric measure of the strength and direction of the monotonic relationship between two variables) ignoring `nan`s **=> feature_21 and feature_31 are highly correlated to weight.**\n",
    "\n",
    "2. TODO: check if dependent on `symbol_id`?\n",
    "\n",
    "3. TODO: check trend w.r.t. `time_id`? (highly related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_feature_corrs = []\n",
    "for feature in feature_cols:\n",
    "    df_pandas = sorted_df_symbol_id.select('weight', feature).collect().to_pandas()\n",
    "    weight_feature_corr = df_pandas['weight'].corr(df_pandas[feature], method='spearman')\n",
    "    weight_feature_corrs.append((feature, weight_feature_corr))\n",
    "\n",
    "highly_correlated = [feature for feature, corr in weight_feature_corrs if np.abs(corr) > 0.7]\n",
    "moderately_correlated = [feature for feature, corr in weight_feature_corrs if 0.3 < np.abs(corr) <= 0.7]\n",
    "\n",
    "print(\"Highly correlated features:\", highly_correlated)\n",
    "print(\"Moderately correlated features:\", moderately_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1x2 grid of subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot between 'weight' and 'feature_21' on the first subplot\n",
    "axes[0].scatter(sorted_df_symbol_id.select(\"weight\").collect().to_pandas(), sorted_df_symbol_id.select(\"feature_21\").collect().to_pandas())\n",
    "axes[0].set_xlabel(\"Weight\")\n",
    "axes[0].set_ylabel(\"Feature 21\")\n",
    "axes[0].set_title(\"Weight vs Feature 21\")\n",
    "\n",
    "# Scatter plot between 'weight' and 'feature_31' on the second subplot\n",
    "axes[1].scatter(sorted_df_symbol_id.select(\"weight\").collect().to_pandas(), sorted_df_symbol_id.select(\"feature_31\").collect().to_pandas())\n",
    "axes[1].set_xlabel(\"Weight\")\n",
    "axes[1].set_ylabel(\"Feature 31\")\n",
    "axes[1].set_title(\"Weight vs Feature 31\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot for Feature 21\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.plot(sorted_df_symbol_id.select(\"feature_21\").collect().to_pandas(), label=\"Feature 21\", color=\"blue\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature 21 Value\")\n",
    "plt.title(\"Plot of Feature 21\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot for Feature 31\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.plot(sorted_df_symbol_id.select(\"feature_31\").collect().to_pandas(), label=\"Feature 31\", color=\"green\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature 31 Value\")\n",
    "plt.title(\"Plot of Feature 31\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between `symbol_id` and `weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average weight for each symbol_id\n",
    "avg_weight = train_df.group_by(\"symbol_id\").agg(\n",
    "    pl.col(\"weight\").mean().alias(\"avg_weight\")\n",
    ").collect().to_pandas()\n",
    "\n",
    "# plot average weight for each symbol_id\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=avg_weight[\"symbol_id\"], y=avg_weight[\"avg_weight\"])\n",
    "plt.xlabel(\"Symbol ID\")\n",
    "plt.ylabel(\"Average Weight\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average Weight by Symbol ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between `date_id`/`time_id` and `weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average weight by date_id\n",
    "avg_weight = train_df.group_by(\"date_id\").agg(\n",
    "    pl.col(\"weight\").mean().alias(\"avg_weight\")\n",
    ").collect().to_pandas()\n",
    "\n",
    "# plot average weight by date_id\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=avg_weight[\"date_id\"], y=avg_weight[\"avg_weight\"])\n",
    "plt.xlabel(\"Date ID\")\n",
    "plt.ylabel(\"Average Weight\")\n",
    "plt.title(\"Average Weight by Date ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation between weight and date_id\n",
    "correlation = train_df.select('weight', 'date_id').collect().to_pandas().corr(method='spearman')\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average weight by time_id\n",
    "avg_weight = train_df.group_by(\"time_id\").agg(\n",
    "    pl.col(\"weight\").mean().alias(\"avg_weight\")\n",
    ").collect().to_pandas()\n",
    "\n",
    "# plot average weight by time_id\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=avg_weight[\"time_id\"], y=avg_weight[\"avg_weight\"])\n",
    "plt.xlabel(\"Time ID\")\n",
    "plt.ylabel(\"Average Weight\")\n",
    "plt.title(\"Average Weight by Time ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation between weight and time_id\n",
    "correlation = train_df.select('weight', 'time_id').collect().to_pandas().corr(method='spearman')\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. More on Responders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responder_columns = [f\"responder_{i}\" for i in range(9)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(responder_columns):\n",
    "    axes[i].plot(sorted_df_symbol_id.select(column).collect().to_pandas(), label=column)\n",
    "    axes[i].set_xlabel(\"Row Index\")\n",
    "    axes[i].set_ylabel(f\"Responder {i}\")\n",
    "    axes[i].set_title(f\"Responder {i}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize Spearman correlation between responders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = sorted_df_symbol_id.select(responder_columns).collect().to_pandas().corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", \n",
    "            linewidths=0.5, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "plt.title(\"Spearman correlation between responders, symbol_id=8\")\n",
    "plt.xlabel(\"Responders\")\n",
    "plt.ylabel(\"Responders\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too computationally expensive\n",
    "# correlation_matrix = train_df.select(responder_columns).collect().to_pandas().corr(method='spearman')\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", \n",
    "#             linewidths=0.5, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "# plt.title(\"Spearman correlation between responders, symbol_id=8\")\n",
    "# plt.xlabel(\"Responders\")\n",
    "# plt.ylabel(\"Responders\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More on instrument independent analysis (the above analysis is on a single instrument that is randomly selected)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jane-street-rmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
